{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen del libro Deep Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## [Tensores](#Tensores)\n",
    " \n",
    " #### [Escalar](#Escalar)\n",
    " #### [Vector](#Vector)\n",
    " #### [Matriz](#Matriz)\n",
    " #### [Tensor 3-dimensional](#Tensor-3-dimensional)\n",
    " #### [Tensor n-dimensional](#Tensor-n-dimensional)\n",
    " \n",
    " ### [Manipulando tensores](#Manipulando-tensores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estructuras basicas de datos en Deep Learning son los tensores\n",
    "que son basicamente arrays n-dimensionales, por ejemplo una matriz\n",
    "es un tensor 2-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: 27\n",
      "Dimension: 0\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear un escalar (tensor 0-dimensional) en numpy:\n",
    "\n",
    "escalar = np.array(27)\n",
    "print(\"Tensor: \" + str(escalar)) # Nos muestra el escalar que acabamos de crear\n",
    "print(\"Dimension: \" + str(escalar.ndim)) # Nos muestra las dimensiones (el rango o los ejes) del tensor que hemos creado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: [1 2 3 4]\n",
      "Dimension: 1\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear un vector (tensor 1-dimensional) en numpy:\n",
    "\n",
    "vector = np.array([1,2,3,4]) # Creamos un vector 4-dimensional\n",
    "print(\"Tensor: \" + str(vector))\n",
    "print(\"Dimension: \" + str(vector.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: \n",
      "\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "\n",
      " Dimension: 2\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear una matriz (tensor 2-dimensional) en numpy:\n",
    "\n",
    "matriz = np.array([[1,2,3,4],\n",
    "                  [5,6,7,8],\n",
    "                  [9,10,11,12]])\n",
    "print(\"Tensor: \\n\\n \" + str(matriz))\n",
    "print(\"\\n Dimension: \" + str(matriz.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 3-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: \n",
      "\n",
      " [[[ 1  1  2 34  0]\n",
      "  [ 6  2  3 35  1]\n",
      "  [ 7  3  4 36  2]]\n",
      "\n",
      " [[ 2  1  2 34  0]\n",
      "  [ 6  2  3 35  1]\n",
      "  [ 7  3  4 36  2]]\n",
      "\n",
      " [[ 3  1  2 34  0]\n",
      "  [ 6  2  3 35  1]\n",
      "  [ 7  3  4 36  2]]]\n",
      "\n",
      " Dimension: 3\n"
     ]
    }
   ],
   "source": [
    "#Vamos a crear un tensor 3-dimensional en numpy:\n",
    "\n",
    "tensor3d = np.array([  [[1, 1, 2, 34, 0],\n",
    "                       [6, 2, 3, 35, 1],\n",
    "                       [7, 3, 4, 36, 2]],\n",
    "              \n",
    "                       [[2, 1, 2, 34, 0],\n",
    "                       [6, 2, 3, 35, 1],\n",
    "                       [7, 3, 4, 36, 2]],\n",
    "                \n",
    "                       [[3, 1, 2, 34, 0],\n",
    "                       [6, 2, 3, 35, 1],\n",
    "                       [7, 3, 4, 36, 2]]   ])\n",
    "\n",
    "print(\"Tensor: \\n\\n \" + str(tensor3d))\n",
    "print(\"\\n Dimension: \" + str(tensor3d.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores n-dimensionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede que nos hayamos dado cuenta de un patron. Aumentar de dimension un tensor no es mas\n",
    "que meter varios tensores de la dimension anterior en un array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulando tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para **crear** tensores es como hemos visto arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "tensor = np.array([1,2])\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver la **forma** que tiene un tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "forma = tensor3d.shape\n",
    "print(forma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver el **tipo** de un tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "tipo = tensor3d.dtype\n",
    "print(tipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un slicing normal y corriente, pero hay que tener en cuenta la n-dimensionalidad del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tensor3d_slice1: \n",
      "\n",
      "[[[1 1]]]\n",
      "---------------------\n",
      "tensor3d_slice2: \n",
      "\n",
      "[[[1 1]\n",
      "  [6 2]]\n",
      "\n",
      " [[2 1]\n",
      "  [6 2]]]\n",
      "---------------------\n",
      "tensor3d_slice3: \n",
      "\n",
      "[[[1 1]]\n",
      "\n",
      " [[2 1]]\n",
      "\n",
      " [[3 1]]]\n"
     ]
    }
   ],
   "source": [
    "tensor3d_slice1 = tensor3d[0:1,0:1,0:2] # De la primera matriz coge la primera fila, los dos primeros elementos.\n",
    "tensor3d_slice2 = tensor3d[0:2,0:2,0:2] # De las 2 primeras matrices coge las 2 primeras filas, y de estas los dos primeros elementos.\n",
    "tensor3d_slice3 = tensor3d[0:3,0:1,0:2] # De las 3 primeras matrices coge la primera fila, los dos primeros elementos.\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"tensor3d_slice1: \\n\")\n",
    "print(tensor3d_slice1)\n",
    "print(\"---------------------\")\n",
    "print(\"tensor3d_slice2: \\n\")\n",
    "print(tensor3d_slice2)\n",
    "print(\"---------------------\")\n",
    "print(\"tensor3d_slice3: \\n\")\n",
    "print(tensor3d_slice3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los indices del slicing se van seleccionando de fuera hacia dentro.\n",
    "en el ejemplo ` tensor3d[0:3,0:1,0:2] ` hemos seleccionado las 3 primeras matrices,\n",
    "de estas tres matrices hemos cogido de cada una la primera fila, y de cada una de las filas que\n",
    "hemos cogido solo nos quedamos con los 2 primeros elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Deep Learning consideramos el primer indice (el primer eje) como el **eje de los ejemplos**\n",
    ",ya que es el indice del conjunto mas exterior que engloba todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lotes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente en Deep Learning los ejemplos se usan por **lotes** (batchs en ingles), es decir,\n",
    "separan todos los ejemplos que tenemos en dos o mas lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores en el mundo real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectores de datos ->** tensores 2-d de la forma (ejemplos, caracteristicas)\n",
    "\n",
    "**Series temporales ->** tensores 3-d de la forma (ejemplos, tiempo, caracteristicas).\n",
    "\n",
    "**Imagenes ->** tensores 4-d de la forma (ejemplos, altura, anchura, canales) o de la forma (ejemplos, canales, altura, anchura)\n",
    "\n",
    "**Videos ->** tensores 5-d de la forma (ejemplos, frames, altura, anchura, canales) o de la forma (ejemplos, frames, canales, altura, anchura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones con tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacion ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    \n",
    "    assert len(x.shape)==2 # Nos aseguramos antes de empezar el programa que\n",
    "                           # lo que le pasamos es un tensor 2-d\n",
    "    \n",
    "    x = x.copy() # Evitamos modificar el tensor original\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] = max(x[i,j],0)\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suma de tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x,y):\n",
    "    \n",
    "    assert len(x.shape)==2 # Nos aseguramos de que los dos elementos que\n",
    "    assert len(y.shape)==2 # vamos a sumar son tensores 2-d\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[i,j]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabiendo como es la suma podemos implementar facilmente la resta, multiplicacion y la division. Solo hay que cambiar que operacion hacemos con los elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suma de tensores de distinta dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    \n",
    "    assert len(x.shape) == 2 # Una matriz\n",
    "    assert len(y.shape) == 1 # Un vector\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            \n",
    "            x[i, j] += y[j] # A todos los elementos de la misma columna\n",
    "                            # les vamos sumando cada elemento correspondiente\n",
    "                            # del vector\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto escalar entre vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    \n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "\n",
    "    z = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "        \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto escalar entre matriz y vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada elemento de z es el producto escalar entre cada fila de la matriz y el vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    \n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "            \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto escalar entre matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    \n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0] # Solo se puede hacer si la primera\n",
    "                                    # matriz tiene las mismas columnas\n",
    "                                    # como filas tiene la segunda\n",
    "            \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo esto se hace mucho mas simple con numpy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suma de tensores en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "xs,ys = np.array([1,2,3]),np.array([4,5,6])\n",
    "\n",
    "suma = xs + ys # Da igual si son de distinta dimension\n",
    "print(suma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operacion ReLu en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "xr = np.array([1,-1,0])\n",
    "\n",
    "relu = np.maximum(xr, 0)\n",
    "print(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto escalar en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "xe,ye = np.array([1,2,3]),np.array([4,5,6])\n",
    "\n",
    "prod_esc = np.dot(xe, ye)\n",
    "print(prod_esc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ejemplo de forma](producto_escalar_matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remodelado de tensores en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin remodelar: \n",
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]]\n",
      "\n",
      " Remodelado: \n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "xre = np.array([[0., 1.],\n",
    "                [2., 3.],\n",
    "                [4., 5.]])\n",
    "print(\"Sin remodelar: \\n\" + str(xre))\n",
    "\n",
    "remodelado = xre.reshape((6,1))\n",
    "print(\"\\n Remodelado: \\n\" + str(remodelado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpuesta de una matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz: \n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "\n",
      " Transpuesta: \n",
      "[[ 1  5  9]\n",
      " [ 2  6 10]\n",
      " [ 3  7 11]\n",
      " [ 4  8 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matriz: \\n\" + str(matriz))\n",
    "\n",
    "transpuesta = matriz.transpose()\n",
    "print(\"\\n Transpuesta: \\n\" + str(transpuesta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El motor de las Redes Neuronales:  optimizacion basada en gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la base del Machine Learning esta el *bucle de entrenamiento*:\n",
    "\n",
    "1- Coge un lote de ejemplos que seran entrenamiento y otro para targets.  \n",
    "\n",
    "2- Ejecuta la red sobre entrenamiento para obtener predicciones sobre los targets.  \n",
    "\n",
    "3- Calcula la diferencia entre lo que deberia haber predecido y lo que ha predecido.  \n",
    "\n",
    "4- Actualiza los pesos de la red para minimizar la diferencia.\n",
    "\n",
    "Este bucle es basicamente lo que permite a nuestro modelo aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos estos pasos son relativamente sencillos, excepto el cuarto, para el que usaremos el **gradiente**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que definir el concepto de **derivada**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Derivada de f en p](derivada_de_f_en_p.png)\n",
    "Derivada de f en p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivada nos inidica **como varia** y en funcion de x.\n",
    "Para que una funcion sea derivable tiene que ser **continua** y **suave**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente: derivada de una operacion de tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es basicamente la derivada de una funcion en un tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradiente de un tensor](gradiente_de_un_tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos un numero que nos indique cuanto nos movemos en en el gradiente.\n",
    "A este numero normalmente se le llama ***Learning Rate***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso por el gradiente estocastico (aleatorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Coge un lote de los ejemplos de entrenamiento x y sus correspondientes targets.  \n",
    "\n",
    "2- Ejecuta la red sobre x para obtener *y_preds*.  \n",
    "\n",
    "3- Calcula la diferencia entre lo predecido y lo que deberiamos haber predecido.  \n",
    "\n",
    "4- Calcula el gradiente de esa diferencia, considerando los parametros de la red.  \n",
    "\n",
    "5- Modifica los parametros de la red en direccion opuesta al gradiente, por ejemplo `W-= step * gradient` para reducir la diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cogiesemos un elemento solo por cada iteracion, estariamos hablando de ***true stochastic gradient descent***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cogiesemos todos los datos en cada iteracion, hablariamos de ***batch stochastic gradient descent***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradiente en 2D](gradiente_2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podriamos deducir, podemos caer en un minimo o maximo local. Para evitar esto usamos el *momentum* (basandonos en la velocidad o en la pendiente actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo seria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`past_velocity = 0.\n",
    "momentum = 0.1\n",
    "while loss > 0.01:\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum + learning_rate * gradient\n",
    "    w=w+ momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de propagacion hacia atras: encadenando derivadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal es basicamente **varias operaciones de tensores encadenadas**, en la que cada una tiene una derivada conocida.\n",
    "Por ejemplo una red f compuesta de tres operaciones de tensores a, b y c con matrices de pesos W1, W2 y W3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empieza por la diferencia que resulta entre lo predecido y lo que deberiamos haber predecido, y va desde la ultima hasta la primera capa, aplicando la **regla de la cadena** para calcular la contribucion que hace cada parametro en dicha diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos tensorflow y keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from plaidml import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nucleo de las redes neuronales se basa en las **capas**, que las podemos considerar como una especie de filtros de datos.  \n",
    "Estos datos entran en la red, y salen de una forma en la que nos resulta mas util. En concreto, las capas extraen representaciones de los datos.  \n",
    "Hay varios tipos de capas, pero ahora mismo no nos interesa eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_ellesmere.0\"\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos la red, pero le tenemos que meter tambien:\n",
    "- **optimizer** (el mecanismo por el que la red va a ir cambiando sus pesos)\n",
    "- **loss function** (basicamente como se calcula la diferencia entre lo obtenido y lo esperado)\n",
    "- **metrics** (para monitorizar la red durante el entrenamiento y el testeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de pasarle los datos a la red, los tenemos que poner en la **forma en la que la red lo espera**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien necesitamos **categorizar** las etiquetas (se explicara mas tarde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora **entrenamos** la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2572 - acc: 0.9263\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1033 - acc: 0.9693\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0684 - acc: 0.9797\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0493 - acc: 0.9856\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0370 - acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efbbc076f50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo calculamos la **accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/step\n",
      "test_acc: 0.9794\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tyk2]",
   "language": "python",
   "name": "conda-env-.conda-tyk2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
